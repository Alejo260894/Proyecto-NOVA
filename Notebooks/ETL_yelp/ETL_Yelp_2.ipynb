{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "from fuzzywuzzy import fuzz,process\n",
    "import functools\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de los archivos Yelp\n",
    "Ruta_archivos_Yelp = '../../Data/yelp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ETL de Archivos Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Archivo checkin.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para abrir chekin de yelp\n",
    "\n",
    "def abrir_Archivo_json(archivo):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    with open(archivo) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                merged_data.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al decodificar JSON en el archivo {archivo}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "archivo = os.path.join(Ruta_archivos_Yelp, 'checkin.json')\n",
    "df_checkin_yelp = abrir_Archivo_json(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos si tiene nulos\n",
    "df_checkin_yelp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos si tiene duplicados\n",
    "df_checkin_yelp.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desanidamos la fecha (en Fecha y hora)\n",
    "desanidados = []\n",
    "\n",
    "for _, row in df_checkin_yelp.iterrows():\n",
    "    business_id = row['business_id']\n",
    "    dates = row['date'].split(', ')\n",
    "    for date in dates:\n",
    "        date, time = date.split(' ')\n",
    "        desanidados.append([business_id, time, date])\n",
    "\n",
    "df_desanidado = pd.DataFrame(desanidados, columns=['business_id', 'hour', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el campo 'date' como datetime y agregamos los campos year y month\n",
    "df_desanidado['date'] = pd.to_datetime(df_desanidado['date'])\n",
    "df_desanidado['year'] = df_desanidado['date'].dt.year\n",
    "df_desanidado['month'] = df_desanidado['date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "df_desanidado.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a CSV\n",
    "#df_desanidado.to_csv('../Salidas/Dataset_Checkin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a PARQUET\n",
    "#df_desanidado.to_parquet('../../Data/data_procesada/Dataset_Checkin.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Archivo user.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo\n",
    "archivo = os.path.join(Ruta_archivos_Yelp, \"user.parquet\")\n",
    "df_User_yelp = pd.read_parquet(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos duplicados y reindexamos\n",
    "df = df_User_yelp.drop_duplicates(subset=[\"user_id\", \"name\"], keep=\"first\")\n",
    "df_User_yelp = df_User_yelp.reindex(df.index)\n",
    "df_User_yelp['id_user'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "df_User_yelp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos y exportamos un nuevo dataframe para conservar id_user original con su user_id , el cual será usado para unir tablas\n",
    "dfusuario = df_User_yelp.loc[:, [\"id_user\",'user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renombramos la columna user_id\n",
    "dfusuario.rename(columns={\"user_id\": \"yelp_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "dfusuario.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportamos a CSV\n",
    "#dfusuario.to_csv(\"../Salidas/UsuarioYelp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna user_id del dataframe df_user_yelp\n",
    "df_User_yelp.drop(columns=['user_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos transformaciones a los tipos de datos en las diferentes variables\n",
    "\n",
    "# Cambiamos a string la variable name\n",
    "df_User_yelp['name'] = df['name'].astype(str)\n",
    "\n",
    "# Cambiamos a tipo datetime la variable yelping_since\n",
    "df_User_yelp['yelping_since'] = pd.to_datetime(df['yelping_since'])\n",
    "\n",
    "# Creamos una columna año a partir de elite que devuelva una lista de años separado por comas\n",
    "df_User_yelp['years'] = df['elite'].str.split(',')\n",
    "\n",
    "# Creamos un nuevo DataFrame con filas individuales para cada año y usuario\n",
    "years_df = df_User_yelp[['id_user', 'years']].explode('years')\n",
    "\n",
    "# Reemplazamos valores vacios por np.nan en years\n",
    "years_df['years'] = years_df['years'].replace(\"\", np.nan)\n",
    "\n",
    "# Reemplazamos 20 po 2020 en years\n",
    "years_df['years'] = years_df['years'].replace(\"20\", \"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventimos el campo years a int convirtiendo valores no válidos a NaN\n",
    "years_df['years'] = pd.to_numeric(years_df['years'], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a CSV\n",
    "#years_df.to_csv(\"../Salidas/Dataset_User_Elite.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a PARQUET\n",
    "#years_df.to_parquet(\"../../Data/data_procesada/Dataset_User_Elite.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas elite y years del dataframe original user.parquet\n",
    "df_User_yelp.drop(columns=['elite'], inplace=True)\n",
    "df_User_yelp.drop(columns=['years'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Archivo business.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo\n",
    "archivo = os.path.join(Ruta_archivos_Yelp, 'business.pkl')\n",
    "df_business= pd.read_pickle(archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos nuevas columnas al dataframe con valores iniciales en NONE\n",
    "df_business['NAME']=None\n",
    "df_business['REVIEW_COUNT']=None\n",
    "df_business['POSTAL_CODE']=None\n",
    "df_business['CITY']=None\n",
    "df_business['STATE']=None\n",
    "df_business['BUSINESS_ID']=None\n",
    "df_business['ADDRESS']=None\n",
    "df_business['LATITUDE']=None\n",
    "df_business['LONGITUDE']=None\n",
    "df_business['STARS']=None\n",
    "df_business['IS_OPEN']=None\n",
    "df_business['ATTRIBUTES']=None\n",
    "df_business['CATEGORIES']=None\n",
    "df_business['HOURS']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la actualización de la columna \"NAME\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.\n",
    "for index,i in enumerate(df_business.name.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'NAME'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la actualización de la columna \"CITY\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.\n",
    "for index,i in enumerate(df_business.city.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'CITY'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la actualización de la columna \"STATE\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.\n",
    "for index,i in enumerate(df_business.state.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'STATE'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este script, se actualiza la columna 'REVIEW_COUNT' del DataFrame df_business con el primer número entero encontrado en cada valor, recorriendo los valores de la columna y almacenando los números enteros en una lista antes de asignarlos a la columna.\n",
    "for index,i in enumerate(df_business.review_count.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,int):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'REVIEW_COUNT'] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se fusionan los caracteres de texto presentes en cada valor de la columna 'business_id' del DataFrame df_b, actualizando la columna \"BUSINESS_ID\" con los valores resultantes\n",
    "for index,i in enumerate(df_business.business_id.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'BUSINESS_ID'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se fusionan los caracteres de texto presentes en cada valor de la columna 'address' del DataFrame 'df_business', actualizando la columna 'ADDRESS' con los valores resultantes\n",
    "for index,i in enumerate(df_business.address.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'ADDRESS'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se fusionan los caracteres de texto presentes en cada valor de la columna 'postal_code' del DataFrame 'df_business', actualizando la columna 'POSTAL_CODE' con los valores resultantes\n",
    "for index,i in enumerate(df_business.postal_code.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'POSTAL_CODE'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se filtran los elementos numéricos mayores que 1 en cada valor de la columna 'latitude' del DataFrame 'df_business'. A continuación, se asigna el primer elemento filtrado a la columna 'LATITUDE' en el DataFrame 'df_business' en la fila correspondiente al índice actual.\n",
    "for index,i in enumerate(df_business.latitude.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e>1:\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'LATITUDE'] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se filtran los elementos numéricos menores que -1 en cada valor de la columna 'longitude' del DataFrame 'df_business'. Luego, se asigna el primer elemento filtrado a la columna 'LONGITUDE' en el DataFrame 'df_business' en la fila correspondiente al índice actual.\n",
    "for index,i in enumerate(df_business.longitude.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if e<-1:\n",
    "            arr.append(e)\n",
    "    df_business.loc[index, 'LONGITUDE'] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se filtran los elementos numéricos mayores que 0.1 en cada valor de la columna 'stars' del DataFrame 'df_business'. Después, se asigna el primer elemento filtrado a la columna 'STARS' en el DataFrame 'df_business' en la fila correspondiente al índice actual.\n",
    "for index,i in enumerate(df_business.stars.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e>0.1:\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'STARS'] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se filtran los elementos numéricos mayores o iguales a 0 en cada valor de la columna 'is_open' del DataFrame 'df_business'. A continuación, se asigna el primer elemento filtrado a la columna 'IS_OPEN' en el DataFrame 'df_business' en la fila correspondiente al índice actual\n",
    "for index,i in enumerate(df_business.is_open.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e >=0:\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'IS_OPEN'] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este código busca y guarda el primer diccionario encontrado en la columna 'attributes' del dataframe 'df_business' en la columna 'ATTRIBUTES'.\n",
    "# No se puede evitar los warnings porque por alguna extraña razón df_business.loc[index,'ATTRIBUTES']=arr[0] no funciona\n",
    "for index,i in enumerate(df_business.attributes.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,dict):\n",
    "            arr.append(e)\n",
    "    if len(arr)>0:\n",
    "        df_business['ATTRIBUTES'][index] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este código filtra los elementos de tipo cadena en la columna 'categories' del dataframe 'df_business'. Luego, fusiona todas las cadenas filtradas en una sola cadena y la asigna a la columna 'CATEGORIES' en el dataframe 'df_business'.\n",
    "for index,i in enumerate(df_business.categories.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business.loc[index, 'CATEGORIES'] = ''.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este código, se recorren los valores de la columna 'hours' en el dataframe 'df_business' y se filtran los elementos que son diccionarios. Luego, se asigna el primer diccionario encontrado a la columna 'HOURS' en el dataframe 'df_business' en la fila correspondiente. En resumen, el código extrae y guarda el primer diccionario encontrado en la columna 'hours' del dataframe 'df_business' en la columna 'HOURS'.\n",
    "# No se puede evitar los warnings porque por alguna extraña razón df_business.loc[index,'HOURS']=arr[0] no funciona\n",
    "for index,i in enumerate(df_business.hours.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,dict):\n",
    "            arr.append(e)\n",
    "    if len(arr)>0:     \n",
    "        df_business['HOURS'][index] = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el dataframe solo a las columnas nuevas generadas con los bucles, con los nombres iniciales\n",
    "df_business=df_business[['BUSINESS_ID','NAME','REVIEW_COUNT','CITY','STATE','ADDRESS','POSTAL_CODE','LATITUDE','LONGITUDE','STARS','IS_OPEN','ATTRIBUTES','CATEGORIES','HOURS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.rename(columns = {'BUSINESS_ID':'business_id',\n",
    "                              'NAME':'name',\n",
    "                              'ADDRESS':'address',\n",
    "                              'CITY':'city',\n",
    "                              'STATE':'state',\n",
    "                              'POSTAL_CODE':'postal_code',\n",
    "                              'LATITUDE':'latitude',\n",
    "                              'LONGITUDE':'longitude',\n",
    "                              'STARS':'stars',\n",
    "                              'REVIEW_COUNT':'review_count',\n",
    "                              'IS_OPEN':'is_open',\n",
    "                              'ATTRIBUTES':'attributes',\n",
    "                              'CATEGORIES':'categories',\n",
    "                              'HOURS':'hours'}, inplace = True)\n",
    "df_business = df_business[['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
    "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
    "       'attributes', 'categories', 'hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2 = df_business.copy()\n",
    "df_business2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos un nuevo CSV con las ciudades de Estados Unidos\n",
    "# Fuente: https://simplemaps.com/data/us-cities\n",
    "city = pd.read_csv('../../Data/data_extra/uscities.csv')\n",
    "city.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudades unicas de estados unidos\n",
    "ciudades_estados_unidos = city['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la primera función busca la mejor coincidencia de una ciudad en una lista, mientras que la segunda función calcula un umbral de similitud basado en la longitud de una ciudad. Ambas funciones se utilizan en conjunto para determinar la mejor coincidencia de una ciudad y aplicar un criterio de aceptación basado en el umbral de similitud.\n",
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  \n",
    "def encontrar_mejor_coincidencia(ciudad):\n",
    "    mejor_coincidencia = process.extractOne(ciudad, ciudades_estados_unidos)\n",
    "    resultado = mejor_coincidencia[0] if mejor_coincidencia[1] >= calcular_umbral_similitud(len(ciudad)) else ciudad\n",
    "    return resultado\n",
    "\n",
    "def calcular_umbral_similitud(longitud_ciudad):\n",
    "    umbral_base = 55\n",
    "    umbral = umbral_base - (longitud_ciudad // 3)\n",
    "    return max(umbral, umbral_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos la funcion\n",
    "encontrar_mejor_coincidencia('nw york')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se aplica la función encontrar_mejor_coincidencia a cada valor de la columna 'city' del DataFrame df_b. La función busca la mejor coincidencia de cada ciudad en una lista de ciudades de EE.UU. y actualiza la columna 'city' con las mejores coincidencias encontradas.\n",
    "# Demora como 17-23 min\n",
    "df_business2['city'] = df_business2['city'].apply(encontrar_mejor_coincidencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un dataframe desde city con las columnas City y State solamente\n",
    "citystate = city[['city', 'state_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se fusionan dos DataFrames ('df_business' y 'citystate') por la columna 'city' en una unión izquierda.\n",
    "df_business2 = df_business2.merge(citystate, left_on='city', right_on='city', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna duplicada 'state'\n",
    "df_business2.drop('state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se actualiza el dataframe df_business, manteniendo solo las columnas mencionadas.\n",
    "df_business2 = df_business2[['business_id', 'name', 'address', 'city', 'state_name',\n",
    "       'postal_code', 'latitude', 'longitude', 'stars', 'review_count',\n",
    "       'is_open', 'attributes', 'categories', 'hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos la columna state_name a state\n",
    "df_business2 = df_business2.rename(columns={'state_name':'state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el dataframe con los 5 estados que se usarán en el proyecto\n",
    "# statein = ['New York' , 'Florida' , 'California', 'Nevada' , 'Hawaii']\n",
    "statein = ['New York' , 'Florida' , 'California', 'Nevada' , 'Hawaii', \"Connecticut\", \"New Jersey\", \"Texas\", \"Pennsylvania\", \"Alaska\"]\n",
    "df_business2 = df_business2[df_business2['state'].isin(statein)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print error comprobar huwaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renombramos index\n",
    "df_business2 = df_business2.rename(columns={'index':'id_business'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo dataframe solo con las columnas 'id_business','business_id'\n",
    "#Creamos la tabla de dimension BusinessYelp\n",
    "BusinessYelp = df_business2[['id_business', 'business_id']]\n",
    "\n",
    "# Renombrar la columna \"business_id\" como \"businessYelp_id\"\n",
    "BusinessYelp.rename(columns={\"business_id\": \"businessYelp_id\"}, inplace=True)\n",
    "BusinessYelp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos la tabla\n",
    "#BusinessYelp.to_csv('../Salidas/BusinessYelpId.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos por ultima vez el dataframe\n",
    "df_business2 = df_business2[['id_business','name', 'address', 'city', 'state',\n",
    "       'postal_code', 'latitude', 'longitude', 'stars', 'review_count',\n",
    "       'is_open', 'attributes', 'categories', 'hours']]\n",
    "df_business2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a númerico la columna postal_code\n",
    "df_business2[\"postal_code\"] = df_business2[\"postal_code\"].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe auxiliar  que contiene solo las filas donde el valor de la columna 'state' coincide con \"New York\", \"Florida\", \"California\", \"Nevada\", \"Hawaii\", \"Connecticut\", \"New Jersey\", \"Texas\", \"Pennsylvania\" o \"Alaska\".\n",
    "aux = df_business2.query('state == \"New York\" or state == \"Florida\" or state == \"California\" or state == \"Nevada\" or state == \"Hawuai\" or state == \"Connecticut\" or state == \"New Jerse\" or state == \"Texas\" or state == \"Pennsylvania\" or state == \"Alaska\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos filas específicas en el DataFrame y realizamos cambios en las columnas 'state' y 'postal_code' en función de los valores de las columnas 'city' y 'postal_code'.\n",
    "df_business2.loc[df_business2[\"city\"] == \"Saint Petersburg\", \"city\"] = \"St. Petersburg\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"St. Petersburg\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"St. Petersburg\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33707\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Tampa\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Tampa\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33610\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Santa Barbara\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"California\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Santa Barbara\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 93101\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Pasco\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Pasco\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos df_business2 a CSV\n",
    "#df_business2.to_csv('../Salidas/Dataset_Business.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos df_business2 a PARQUET\n",
    "df_business2.to_parquet('../../Data/data_procesada/Dataset_Business.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos algunas columnas del dataframe auxiliar\n",
    "aux.drop(columns=['is_open'], inplace=True)\n",
    "aux.drop(columns=['postal_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos duplicados de la columna auxiliar y reindexamos\n",
    "aux = aux.drop_duplicates(subset=[\"id_business\", \"name\"], keep=\"first\")\n",
    "aux['id_business'] = aux.index\n",
    "aux.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un nuevo dataframe con \"id_business\",'categories'\n",
    "\n",
    "#Tabla de dimension de categorias\n",
    "df_cat = aux.loc[:, ['id_business','categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los valores de la columna \"categories\" por coma y expandirlos en filas\n",
    "df_cat['categories'] = df_cat['categories'].str.split(',')\n",
    "df_cat = df_cat[['id_business', 'categories']].explode('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un array con las categorias unicas\n",
    "categorias = df_cat[\"categories\"].unique()\n",
    "# Creamos la tabla de dimension de categorias de yelp\n",
    "df_categorias = pd.DataFrame(categorias, columns=[\"Descripcion\"])\n",
    "df_categorias['IdCategoria'] = df_categorias.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos la tabla de dimensiones de categorias\n",
    "#df_categorias.to_csv(\"../Salidas/BusinessCategorias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nuevo dataframe auxiliar para el detalle de las categorias a partir de df_Cat y df_categorias\n",
    "df_aux = df_cat.merge(df_categorias, left_on=\"categories\", right_on=\"Descripcion\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas categories y Descripcion para generar una tabla de dimension entre id_business y idcategoria\n",
    "df_aux.drop(columns=[\"categories\"],inplace=True)\n",
    "df_aux.drop(columns=[\"Descripcion\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos tabla de dimensiones de detalle de categorias\n",
    "#df_aux.to_csv(\"../Salidas/BusinessDetalleCategorias.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos categories de aux\n",
    "aux.drop(columns=[\"categories\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos tabla de dimension atributos a partir de aux\n",
    "df_atribute = aux.loc[:, [\"id_business\",'attributes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamos y dividimos los elementos de la columna 'attributes'\n",
    "for index, i in enumerate(df_atribute['attributes']):\n",
    "    if isinstance(i,str):\n",
    "       df_atribute['attributes'][index] = i[1:-1].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expandimos la columna 'attributes' del DataFrame 'df' en filas separadas, manteniendo los valores correspondientes de la columna 'id_business'.\n",
    "df_atribute = df_atribute[['id_business', 'attributes']].explode('attributes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos los corchetes y comillas dobles de la columna 'attributes' del DataFrame, dejando los valores limpios y sin esos caracteres específicos\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('{', '').str.replace('}', '')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos los valores en la columna 'attributes', actualizando ciertos aspectos relacionados con estacionamiento de negocios\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('street: True', 'BusinessParking street: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('street: False', 'BusinessParking street: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('validated: True', 'BusinessParking validated: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('validated: False', 'BusinessParking validated: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('lot: True', 'BusinessParking lot: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('lot: False', 'BusinessParking lot: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('valet: True', 'BusinessParking valet: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('valet: False', 'BusinessParking valet: False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo DataFrame llamado 'df_atributos' que contiene los valores únicos de la columna 'attributes' del DataFrame original 'df_atribute', junto con una columna de identificación única para cada valor.\n",
    "atributos = df_atribute[\"attributes\"].unique()\n",
    "df_atributos = pd.DataFrame(atributos, columns=[\"Descripcion\"])\n",
    "df_atributos['IdAtributos'] = df_atributos.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos tabla de atributos\n",
    "#df_atributos.to_csv(\"../Salidas/BusinessAtributos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas \"attributes\" de aux\n",
    "aux.drop(columns=[\"attributes\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la tabla de dimensiones \"Hours\" a partir de aux\n",
    "df_hours = aux.loc[:, [\"id_business\",'hours']]\n",
    "df_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos la tabla de Hours\n",
    "#df_hours.to_csv(\"../Salidas/BusinessHoras.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe df_Hour_detalle\n",
    "df_hour_detalle = df_business2[['id_business','hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_detalle = df_hour_detalle.explode('hours')\n",
    "df_hour_detalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe llamado hour con las horas unicas de df_hour_detalle\n",
    "hours = df_hour_detalle['hours'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un nuevo DataFrame que contiene una sola columna llamada \"Descripcion\" que contiene los valores de la columna 'hours' y reindexamos\n",
    "DescHour = pd.DataFrame(hours,columns=[\"Descripcion\"])\n",
    "DescHour['id_hour'] = DescHour.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos el dataframe BusinessHorarios\n",
    "#DescHour.to_csv('../Salidas/BusinessHorarios.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una union entre df_hour_detalle y DescHour\n",
    "df_hour_detalle.merge(DescHour, left_on='hours', right_on='Descripcion', how='inner')[['id_business','id_hour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos el dataframe df_hour_detalle\n",
    "#df_hour_detalle.to_csv('../Salidas/BusinessDetallesHora.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe df_detalleatributo a partir de la union entre df_atribute y df_atributos\n",
    "df_detalleatributo = df_atribute.merge(df_atributos, left_on=\"attributes\", right_on=\"Descripcion\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas \"attributes\" y \"Descripcion\" de df_detalleatributo\n",
    "df_detalleatributo.drop(columns = \"attributes\", inplace=True)\n",
    "df_detalleatributo.drop(columns = \"Descripcion\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos dataframe df_detalleatributo\n",
    "#df_detalleatributo.to_csv(\"../Salidas/BusinessDetalleAtributos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna hours de aux\n",
    "aux.drop(columns=[\"hours\"],inplace=True)\n",
    "aux.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 tip.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el archivo json\n",
    "json_objects=[]\n",
    "\n",
    "archivo = os.path.join(Ruta_archivos_Yelp, 'tip.json')\n",
    "with open(archivo, 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_objects.append(json.loads(line))\n",
    "\n",
    "df_tip = pd.DataFrame(json_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos un DataFrame que contenga solo las filas correspondientes a los negocios presentes en BusinessYelp\n",
    "df_tip = df_tip[df_tip['business_id'].isin(BusinessYelp.businessYelp_id.unique().tolist())]\n",
    "df_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una unión df_tip con BusinessYelp\n",
    "df_tip = BusinessYelp.merge(df_tip, left_on='businessYelp_id', right_on='business_id', how='right')[['user_id', 'id_business', 'text', 'date', 'compliment_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una unión df_tip con UsuarioYelp\n",
    "df_tip = dfusuario.merge(df_tip, left_on='yelp_id', right_on='user_id', how='right')[['id_user', 'id_business', 'text', 'date', 'compliment_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a CSV\n",
    "#df_tip.to_csv('../Salidas/Dataset_Tip.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a PARQUET\n",
    "df_tip.to_parquet('../../Data/data_procesada/Dataset_Tip.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 review.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo review.json de Yelp\n",
    "jsonarr=[]\n",
    "archivo = os.path.join(Ruta_archivos_Yelp, \"review.json\")\n",
    "with open(archivo, 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        jsonarr.append(json.loads(line))\n",
    "\n",
    "df_review = pd.DataFrame(jsonarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos duplicados y reindexamos\n",
    "df_review = df_review.drop_duplicates(subset=[\"review_id\", \"text\"], keep=\"first\")\n",
    "df_review['id_review'] = df_review.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe con \"id_review\", \"review_id\" llamado df_ReviewYelp\n",
    "df_ReviewYelp = df_review[[\"id_review\", \"review_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos el dataframe ReviewYelpId\n",
    "#df_ReviewYelp.to_csv(\"../Salidas/ReviewYelpId.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columna review_id de df_Review\n",
    "df_review.drop(columns = \"review_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos el dataframe df_review con dfusuario\n",
    "df_review = df_review.merge(dfusuario, left_on=\"user_id\", right_on=\"yelp_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe df_reviewfinal a partir de df_review y BusinessYelp\n",
    "df_reviewfinal = df_review.merge(BusinessYelp, left_on=\"business_id\", right_on=\"businessYelp_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan las columnas \"user_id\" y \"yelp_id\" del dataframe df_reviewfinal\n",
    "df_reviewfinal.drop(columns = \"business_id\", inplace=True)\n",
    "df_reviewfinal.drop(columns = \"businessYelp_id\", inplace=True)\n",
    "df_reviewfinal.drop(columns = \"yelp_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewfinal.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a CSV\n",
    "#df_reviewfinal.to_csv(\"../Salidas/Dataset_Review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos a PARQUET\n",
    "df_reviewfinal.to_parquet(\"../../Data/data_procesada/Dataset_Review.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets exportados disponibles en <https://1drv.ms/f/s!AjBV0Q-Vh1QQmLAZGumu26bVTsnkmw?e=D7EqVJ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pv.read_csv(\"../../Data/data_procesada/Dataset_Business.csv\")\n",
    "pq.write_table(table, \"../../Data/data_procesada/Dataset_Business.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pv.read_csv(\"../../Data/data_procesada/Dataset_Checkin.csv\")\n",
    "pq.write_table(table, \"../../Data/data_procesada/Dataset_Checkin.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv(\"../../Data/data_procesada/Dataset_Review.csv\")\n",
    "table.to_parquet(\"../../Data/data_procesada/Dataset_Review.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv(\"../../Data/data_procesada/Dataset_Tip.csv\")\n",
    "table.to_parquet(\"../../Data/data_procesada/Dataset_Tip.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pv.read_csv(\"../../Data/data_procesada/Dataset_User_Elite.csv\")\n",
    "pq.write_table(table, \"../../Data/data_procesada/Dataset_User_Elite.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview de parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business = pd.read_parquet(\"../../Data/data_procesada/Dataset_Business.parquet\")\n",
    "table_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_indices_hora = table_business['hours'].isnull()\n",
    "null_indices_hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row = table_business.sample()\n",
    "random_hours_value = random_row['hours'].values[0]\n",
    "print(random_hours_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horario_general = {'Friday': '9:0-20:0', 'Monday': '9:0-20:0', 'Saturday': '8:30-15:30', 'Sunday': None, 'Thursday': '9:0-20:0', 'Tuesday': '9:0-20:0', 'Wednesday': '9:0-20:0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business.loc[null_indices_hora, 'hours'] = table_business.loc[null_indices_hora].apply(lambda row: horario_general, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_indices = table_business['hourse'].isnull()\n",
    "null_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "def get_postal_code(latitude, longitude):\n",
    "    location = geocoder.osm([latitude, longitude], method='reverse')\n",
    "    return location.postal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reemplazo los valores nulos en la tabla\n",
    "table_business.loc[null_indices, 'postal_code'] = table_business.loc[null_indices].apply(lambda row: get_postal_code(row['latitude'], row['longitude']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valores_nulos = table_business[table_business['postal_code'].isnull()]\n",
    "valores_nulos\n",
    "#este valor nulo me manda al medio del mar por eso no tiene postal code. Considerar si lo borramos o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faltante = get_postal_code(32.105261,110.942001)\n",
    "print(faltante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business['postal_code'] = table_business['postal_code'].astype(float)\n",
    "#REVISAR MAÑANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = table_business['address'].duplicated(keep=False)\n",
    "duplicated_addresses = table_business[duplicates_mask]\n",
    "duplicated_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business.drop_duplicates(subset=['latitude', 'longitude', 'postal_code'], inplace=True)\n",
    "table_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = table_business.duplicated(subset=['latitude', 'longitude', 'postal_code'], keep=False)\n",
    "num_duplicates = duplicates_mask.sum()\n",
    "num_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# Iterar sobre cada fila del DataFrame original\n",
    "for index, row in table_business.iterrows():\n",
    "    # Obtener las categorías de la fila actual\n",
    "    categories = row['categories']\n",
    "    \n",
    "    # Si hay categorías\n",
    "    if categories:\n",
    "        # Dividir las categorías y eliminar espacios en blanco\n",
    "        categories_list = [category.strip() for category in categories.split(',')]\n",
    "        \n",
    "        # Iterar sobre cada categoría y duplicar la fila actual\n",
    "        for category in categories_list:\n",
    "            # Crear una copia de la fila actual\n",
    "            new_row = row.copy()\n",
    "            # Asignar la categoría actual a la fila copiada\n",
    "            new_row['categories'] = category\n",
    "            # Agregar la fila copiada a la lista\n",
    "            rows.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con las filas duplicadas\n",
    "table_business = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_business.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_checkin = pd.read_parquet(\"../../Data/data_procesada/Dataset_Checkin.parquet\")\n",
    "table_checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_checkin = table_checkin.drop_duplicates(subset=['business_id', 'hour', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_checkin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVIEWS YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_reviews = pd.read_parquet(\"../../Data/data_procesada/Dataset_Review.parquet\")\n",
    "table_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los elementos de la columna \"date\" en cuatro columnas\n",
    "date_split = table_reviews['date'].str.split(' ', expand=True)\n",
    "\n",
    "# Asignar nombres a las nuevas columnas\n",
    "date_split.columns = ['fecha', 'hora']\n",
    "\n",
    "# Dividir la columna \"fecha\" en columnas separadas para año, mes y día\n",
    "date_split[['año', 'mes', 'día']] = date_split['fecha'].str.split('-', expand=True)\n",
    "\n",
    "# Unir las nuevas columnas con el DataFrame original\n",
    "table_reviews = table_reviews.join(date_split[['año', 'mes', 'día', 'hora']])\n",
    "\n",
    "# Eliminar la columna \"date\"\n",
    "table_reviews.drop(columns=['date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar filas duplicadas basadas en múltiples columnas\n",
    "#duplicates = table_reviews.duplicated(subset=['user_id', 'text', 'id_user', 'id_business', 'año', 'mes', 'día'], keep=False)\n",
    "duplicates = table_reviews.duplicated(subset=['user_id', 'id_business', 'año', 'mes', 'día'], keep=False)\n",
    "# Filtrar el DataFrame original para mostrar solo las filas duplicadas\n",
    "duplicated_rows = table_reviews[duplicates]\n",
    "\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_reviews = table_reviews.drop_duplicates(subset=['user_id', 'id_business', 'año', 'mes', 'día'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tip = pd.read_parquet(\"../../Data/data_procesada/Dataset_Tip.parquet\")\n",
    "table_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_nulos = table_tip[table_tip['text'].isnull()]\n",
    "valores_nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = table_tip.duplicated(subset=['id_user', 'id_business', 'text', 'date'], keep=False)\n",
    "# Filtrar el DataFrame original para mostrar solo las filas duplicadas\n",
    "duplicated_rows = table_tip[duplicates]\n",
    "\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tip = table_tip.drop_duplicates(subset=['id_user', 'id_business', 'text', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_tip.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_user = pd.read_parquet(\"../../Data/data_procesada/Dataset_User_Elite.parquet\")\n",
    "table_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = table_user.duplicated(subset=['id_user'], keep=False)\n",
    "# Filtrar el DataFrame original para mostrar solo las filas duplicadas\n",
    "duplicated_rows = table_user[duplicates]\n",
    "\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_user = table_user.drop_duplicates(subset=['id_user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_user.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
